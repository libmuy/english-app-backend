[Script Info]
; Script generated by Aegisub 9706-cibuilds-20caaabc0
; http://www.aegisub.org/
Title: Default Aegisub file
ScriptType: v4.00+
WrapStyle: 0
ScaledBorderAndShadow: yes
YCbCr Matrix: None

[Aegisub Project Garbage]
Audio File: part1.wav
Video Zoom Percent: 1.000000
Active Line: 208

[V4+ Styles]
Format: Name, Fontname, Fontsize, PrimaryColour, SecondaryColour, OutlineColour, BackColour, Bold, Italic, Underline, StrikeOut, ScaleX, ScaleY, Spacing, Angle, BorderStyle, Outline, Shadow, Alignment, MarginL, MarginR, MarginV, Encoding
Style: Default,Arial,48,&H00FFFFFF,&H000000FF,&H00000000,&H00000000,0,0,0,0,100,100,0,0,1,2,2,2,10,10,10,1

[Events]
Format: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text
Dialogue: 0,0:00:00.00,0:00:03.82,Default,,0,0,0,,在过去的一个月里，我一直在玩TensorFlow和Keras，
Dialogue: 0,0:00:03.82,0:00:14.14,Default,,0,0,0,,我注意到很多示例都是预制的演示，非常适合展示各种功能，但并没有真正教你如何使用这些框架。
Dialogue: 0,0:00:14.15,0:00:18.81,Default,,0,0,0,,我希望能改变这一点，而不深入探讨机器学习的内部原理。
Dialogue: 0,0:00:18.81,0:00:23.99,Default,,0,0,0,,在接下来的几集节目中，我将向你展示如何从语音命令中提取特征，
Dialogue: 0,0:00:23.99,0:00:31.31,Default,,0,0,0,,使用这些特征训练深度神经网络，然后使用TensorFlow Lite将该神经网络加载到Raspberry Pi上。
Dialogue: 0,0:00:31.33,0:00:39.24,Default,,0,0,0,,我们的想法是让Pi一直监听音频，当它听到我们的唤醒词“停”时，它会切换一个引脚。
Dialogue: 0,0:00:39.29,0:00:42.52,Default,,0,0,0,,请注意，我并不是机器学习专家。
Dialogue: 0,0:00:42.52,0:00:46.69,Default,,0,0,0,,我也是在学习过程中，但我想分享我到目前为止的发现。
Dialogue: 0,0:00:46.71,0:00:50.49,Default,,0,0,0,,如果你知道任何更好的方法或技术，请在此视频中评论。
Dialogue: 0,0:00:50.49,0:00:53.48,Default,,0,0,0,,我很想知道你是否认为有更好的方法。
Dialogue: 0,0:00:53.58,0:00:57.45,Default,,0,0,0,,大多数机器学习问题可以分为四个步骤。
Dialogue: 0,0:00:57.46,0:01:01.02,Default,,0,0,0,,第一步是收集你认为需要的数据。
Dialogue: 0,0:01:01.04,0:01:08.08,Default,,0,0,0,,对于唤醒词或热词检测，我们需要很多人说不同的词，包括我们的目标词。
Dialogue: 0,0:01:08.10,0:01:11.41,Default,,0,0,0,,好消息是，Google已经为我们完成了这个任务。
Dialogue: 0,0:01:11.42,0:01:14.99,Default,,0,0,0,,我们可以使用他们的语音命令数据集来简化工作。
Dialogue: 0,0:01:14.99,0:01:21.40,Default,,0,0,0,,接下来，我们需要弄清楚可以使用这些数据中的哪些特征来区分不同的命令。
Dialogue: 0,0:01:21.43,0:01:23.79,Default,,0,0,0,,然后，我们将构建并训练一个模型。
Dialogue: 0,0:01:23.80,0:01:33.00,Default,,0,0,0,,最后，我们将该模型部署到Raspberry Pi上，使用TensorFlow Lite帮助我们实时判断是否说出了目标词。
Dialogue: 0,0:01:33.09,0:01:37.43,Default,,0,0,0,,在这一集节目中，我想专注于特征提取。
Dialogue: 0,0:01:37.44,0:01:40.65,Default,,0,0,0,,注意，你不能真正使用TensorFlow Lite训练模型。
Dialogue: 0,0:01:40.65,0:01:46.10,Default,,0,0,0,,它的主要目的是在你有一个完全训练好的模型后进行预测和推理。
Dialogue: 0,0:01:46.12,0:01:52.30,Default,,0,0,0,,所以在前两集节目中，我们将在桌面或笔记本电脑上使用Keras和TensorFlow。
Dialogue: 0,0:01:52.31,0:01:55.98,Default,,0,0,0,,实际上，我们在下一集节目中才会真正使用TensorFlow，
Dialogue: 0,0:01:55.98,0:02:00.98,Default,,0,0,0,,那是因为我们将专注于从音频数据中提取特征。
Dialogue: 0,0:02:00.99,0:02:07.99,Default,,0,0,0,,如果你还没有这样做，我建议你查看我关于安装TensorFlow、Keras和Jupyter Notebook的视频。
Dialogue: 0,0:02:08.00,0:02:10.18,Default,,0,0,0,,在整个过程中我们都需要它们。
Dialogue: 0,0:02:10.18,0:02:17.83,Default,,0,0,0,,如果你有一个Google账户，你也可以前往colab.research.google.com在云端运行Jupyter Notebook。
Dialogue: 0,0:02:17.84,0:02:20.50,Default,,0,0,0,,无需设置且免费使用。
Dialogue: 0,0:02:20.51,0:02:27.96,Default,,0,0,0,,注意，所有文件都存储在你的Google Drive上，因此将大型数据集移入移出工作区可能需要额外工作。
Dialogue: 0,0:02:27.97,0:02:33.85,Default,,0,0,0,,还要注意，将文件加载到Colab中需要使用不同于本地计算机的命令。
Dialogue: 0,0:02:33.95,0:02:36.75,Default,,0,0,0,,首先，我们需要下载数据集。
Dialogue: 0,0:02:36.71,0:02:39.14,Default,,0,0,0,,搜索Google Speech Commands数据集。
Dialogue: 0,0:02:39.30,0:02:43.62,Default,,0,0,0,,我发现最好的办法是到TensorFlow的GitHub仓库文档页面上找。
Dialogue: 0,0:02:43.60,0:02:48.82,Default,,0,0,0,,我会下载版本0.02，这个版本在这个应用中效果很好。
Dialogue: 0,0:02:48.85,0:02:55.08,Default,,0,0,0,,注意，这个下载文件相当大，在下载的同时，你可以为数据集贡献自己的语音。
Dialogue: 0,0:02:55.11,0:02:58.87,Default,,0,0,0,,下载完成后，将文件解压到你计算机上的某个位置。
Dialogue: 0,0:02:58.89,0:03:02.83,Default,,0,0,0,,我会把它放在我的其他Python项目附近的datasets文件夹中。
Dialogue: 0,0:03:02.98,0:03:05.50,Default,,0,0,0,,随意浏览数据集中的文件。
Dialogue: 0,0:03:05.51,0:03:07.67,Default,,0,0,0,,我们将选择“停”作为我们的唤醒词。
Dialogue: 0,0:03:07.70,0:03:13.06,Default,,0,0,0,,如果我们播放它们，它们就是来自不同人说“停”的录音集合。
Dialogue: 0,0:03:13.22,0:03:17.38,Default,,0,0,0,,你需要记录下Speech Commands数据集的绝对路径。
Dialogue: 0,0:03:17.43,0:03:18.75,Default,,0,0,0,,我们一会儿会用到它。
Dialogue: 0,0:03:18.80,0:03:23.70,Default,,0,0,0,,如果你想创建自己的唤醒词，首先需要在这个数据集中创建一个新文件夹。
Dialogue: 0,0:03:23.78,0:03:27.31,Default,,0,0,0,,然后录下你自己或其他人说的那个唤醒词。
Dialogue: 0,0:03:27.34,0:03:32.28,Default,,0,0,0,,你需要确保音频片段正好是一秒钟长，这样才能在这个项目的其余部分中工作。
Dialogue: 0,0:03:32.39,0:03:35.65,Default,,0,0,0,,音频种类越多越好。
Dialogue: 0,0:03:35.62,0:03:39.51,Default,,0,0,0,,理想情况下，你至少需要几百个不同的录音。
Dialogue: 0,0:03:39.51,0:03:44.27,Default,,0,0,0,,这些步骤涉及到很多代码，所以我会简要介绍每段代码。
Dialogue: 0,0:03:44.26,0:03:48.65,Default,,0,0,0,,如果你想仔细研究这些代码，可以在描述中的链接中找到它们。
Dialogue: 0,0:03:48.68,0:03:53.46,Default,,0,0,0,,用TensorFlow或TensorFlow GPU启动一个新的Jupyter Notebook会话。
Dialogue: 0,0:03:53.45,0:03:56.81,Default,,0,0,0,,在新笔记本中，让我们导入必要的包。
Dialogue: 0,0:03:56.79,0:04:02.30,Default,,0,0,0,,我们需要用到OS来处理文件，用Librosa来加载和重采样WAV文件，
Dialogue: 0,0:04:02.31,0:04:08.40,Default,,0,0,0,,用Random来随机数据，用NumPy来处理矩阵，用Matplotlib来绘图，
Dialogue: 0,0:04:08.40,0:04:12.20,Default,,0,0,0,,以及用Python Speech Features来提取音频特征。
Dialogue: 0,0:04:12.20,0:04:14.75,Default,,0,0,0,,注意，如果你没有其中一个包，
Dialogue: 0,0:04:14.73,0:04:20.81,Default,,0,0,0,,可以在Jupyter Notebook中输入感叹号pip install加包名来安装它。
Dialogue: 0,0:04:20.84,0:04:24.87,Default,,0,0,0,,接下来，我们会用join来构建数据集的路径。
Dialogue: 0,0:04:24.88,0:04:28.80,Default,,0,0,0,,让我们打印出来确保可以看到所有的语音目录。
Dialogue: 0,0:04:28.93,0:04:33.37,Default,,0,0,0,,我们可以选择几个词来训练，但我打算用所有的词来训练。
Dialogue: 0,0:04:33.38,0:04:36.82,Default,,0,0,0,,我们会在后面的步骤中告诉它只识别其中一个词。
Dialogue: 0,0:04:36.91,0:04:40.83,Default,,0,0,0,,背景噪音对我来说似乎会搞乱事情，所以我会把它关掉。
Dialogue: 0,0:04:40.85,0:04:48.82,Default,,0,0,0,,理论上，你可以将这些噪音与其他语音样本混合，训练一个可以自动滤除各种背景声音的模型。
Dialogue: 0,0:04:48.83,0:04:52.27,Default,,0,0,0,,让我们打印出每个目标目录中的文件数量。
Dialogue: 0,0:04:52.31,0:04:56.07,Default,,0,0,0,,如你所见，每个词都有几千个样本。
Dialogue: 0,0:04:56.07,0:04:58.91,Default,,0,0,0,,让我们为这个脚本的其余部分设置一些参数。
Dialogue: 0,0:04:58.88,0:05:04.66,Default,,0,0,0,,我们需要为所有目标词创建特征，即使我们稍后只选择一个目标词。
Dialogue: 0,0:05:04.65,0:05:07.84,Default,,0,0,0,,在脚本的最后，我们将有一个特征集合，
Dialogue: 0,0:05:07.81,0:05:13.50,Default,,0,0,0,,基本上类似于图像的矩阵，将它们存储到一个NPZ文件中。
Dialogue: 0,0:05:13.65,0:05:17.29,Default,,0,0,0,,实际上，你会想要使用所有可用的数据。
Dialogue: 0,0:05:17.29,0:05:22.89,Default,,0,0,0,,然而，提取特征和使用100,000个样本进行训练可能需要数小时。
Dialogue: 0,0:05:22.92,0:05:26.94,Default,,0,0,0,,所以我发现使用随机子集数据更容易工作。
Dialogue: 0,0:05:26.92,0:05:28.31,Default,,0,0,0,,我会使用10%。
Dialogue: 0,0:05:28.38,0:05:33.67,Default,,0,0,0,,这只应该用于初步原型，以确保事情正常工作。
Dialogue: 0,0:05:33.76,0:05:38.52,Default,,0,0,0,,当你准备训练最终模型时，你需要回到100%的数据。
Dialogue: 0,0:05:38.59,0:05:44.23,Default,,0,0,0,,我们还需要将10%的数据留作交叉验证和10%的数据留作测试。
Dialogue: 0,0:05:44.23,0:05:45.95,Default,,0,0,0,,我稍后会谈到这一点。
Dialogue: 0,0:05:45.95,0:05:50.23,Default,,0,0,0,,虽然WAV文件是以16kHz的采样率录制的，
Dialogue: 0,0:05:50.27,0:05:57.05,Default,,0,0,0,,但如果我们使用较低的采样率（如8kHz），我们将能够让最终模型运行得更快。
Dialogue: 0,0:05:57.08,0:06:04.51,Default,,0,0,0,,我们将梅尔频率倒谱系数的数量设置为16，这些MFCC的长度为16。
Dialogue: 0,0:06:04.50,0:06:11.37,Default,,0,0,0,,我稍后会谈到为什么MFCC是良好的特征，但让我们继续从文件中获取原始数据。
Dialogue: 0,0:06:11.38,0:06:15.64,Default,,0,0,0,,接下来，我们将创建一个包含全路径的文件名列表。
Dialogue: 0,0:06:15.74,0:06:19.85,Default,,0,0,0,,这将允许我们加载每一个并自动提取特征。
Dialogue: 0,0:06:19.83,0:06:23.06,Default,,0,0,0,,此外，我们需要创建一个Y数组。
Dialogue: 0,0:06:23.05,0:06:26.78,Default,,0,0,0,,这个数组保存了实际值或真实值。
Dialogue: 0,0:06:26.88,0:06:31.44,Default,,0,0,0,,因为这是一个监督学习练习，我们在训练步骤中需要信号的标签。
Dialogue: 0,0:06:31.47,0:06:34.68,Default,,0,0,0,,在训练阶段我们应该为信号添加标签
Dialogue: 0,0:06:34.80,0:06:38.63,Default,,0,0,0,,我们可以任意分配值，但它们应该一致。
Dialogue: 0,0:06:38.67,0:06:41.72,Default,,0,0,0,,我们将按字母顺序分配单词的编号。
Dialogue: 0,0:06:41.81,0:06:46.50,Default,,0,0,0,,Backward是0，bed是1，bird是2，依此类推。
Dialogue: 0,0:06:46.56,0:06:47.96,Default,,0,0,0,,让我们打印出Y。
Dialogue: 0,0:06:47.96,0:06:50.19,Default,,0,0,0,,我们看到它是一个数组集合，
Dialogue: 0,0:06:50.16,0:06:54.54,Default,,0,0,0,,每个数组只是我们分配给目标词的编号。
Dialogue: 0,0:06:54.61,0:06:59.59,Default,,0,0,0,,所以在这个第一个数组中有1664个0，
Dialogue: 0,0:06:59.58,0:07:05.82,Default,,0,0,0,,这对应于1664个说backward的样本。
Dialogue: 0,0:07:05.89,0:07:09.83,Default,,0,0,0,,类似地，在下一个数组中有2014个1，
Dialogue: 0,0:07:09.80,0:07:12.54,Default,,0,0,0,,这对应于bed的样本。
Dialogue: 0,0:07:12.68,0:07:18.15,Default,,0,0,0,,然后我们将这些数组扁平化，使它们成为一个长列表，而不是数组的集合。
Dialogue: 0,0:07:18.16,0:07:21.61,Default,,0,0,0,,接下来的部分最好用图表描述。
Dialogue: 0,0:07:21.73,0:07:27.66,Default,,0,0,0,,我们将使用Python的zip命令将每个文件名与其关联的Y值连接起来。
Dialogue: 0,0:07:27.75,0:07:31.55,Default,,0,0,0,,虽然有更多文件，但请跟着我。
Dialogue: 0,0:07:31.59,0:07:33.83,Default,,0,0,0,,然后我们随机打乱文件名，
Dialogue: 0,0:07:33.83,0:07:37.99,Default,,0,0,0,,注意，Y值仍然链接到各个名称。
Dialogue: 0,0:07:38.06,0:07:44.06,Default,,0,0,0,,然后我们解压这两个列表以分离文件名和Y值，但它们的顺序将保持不变。
Dialogue: 0,0:07:44.12,0:07:50.15,Default,,0,0,0,,最后，我们将前10%的数据留作交叉验证集。
Dialogue: 0,0:07:50.20,0:07:54.92,Default,,0,0,0,,这将在训练期间测试模型的表现时很有用。
Dialogue: 0,0:07:54.91,0:07:58.35,Default,,0,0,0,,然后我们将另外10%的数据留作测试数据。
Dialogue: 0,0:07:58.35,0:08:03.13,Default,,0,0,0,,我们在调整模型和超参数完成后才会使用这些数据进行测试。
Dialogue: 0,0:08:03.20,0:08:06.96,Default,,0,0,0,,我们将剩余的数据作为训练数据。
Dialogue: 0,0:08:07.12,0:08:12.68,Default,,0,0,0,,注意，交叉验证集和测试集可以占总数据的10%到20%。
Dialogue: 0,0:08:12.84,0:08:17.35,Default,,0,0,0,,根据你拥有的总数据量，自由选择该范围内的比例。
Dialogue: 0,0:08:17.34,0:08:21.42,Default,,0,0,0,,回到我们的代码，我们可以看到这些步骤在这些单元格中是如何完成的。
Dialogue: 0,0:08:21.39,0:08:25.86,Default,,0,0,0,,我们将文件名和Y数组一起压缩，打乱它们，然后解压它们。
Dialogue: 0,0:08:25.85,0:08:29.20,Default,,0,0,0,,因为我不想花太多时间训练原型模型，
Dialogue: 0,0:08:29.19,0:08:32.71,Default,,0,0,0,,所以我现在只使用总数据的10%。
Dialogue: 0,0:08:32.70,0:08:39.06,Default,,0,0,0,,当你准备好训练测试和部署的模型时，务必要使用所有的数据。
Dialogue: 0,0:08:39.14,0:08:45.68,Default,,0,0,0,,在这里，我们将文件名列表和真实值列表分解成单独的验证集、测试集和训练集。
Dialogue: 0,0:08:45.84,0:08:48.96,Default,,0,0,0,,现在我们准备从这些WAV文件中提取特征。
Dialogue: 0,0:08:49.00,0:08:54.07,Default,,0,0,0,,在特征提取方面，研究其他人的做法是个好主意。
Dialogue: 0,0:08:54.12,0:08:58.38,Default,,0,0,0,,这就是我发现MFCCs的地方，据我所读到的，
Dialogue: 0,0:08:58.39,0:09:02.49,Default,,0,0,0,,将音频转换为梅尔频率倒谱系数
Dialogue: 0,0:09:02.47,0:09:06.30,Default,,0,0,0,,在语音识别的机器学习中似乎非常流行。
Dialogue: 0,0:09:06.28,0:09:13.58,Default,,0,0,0,,为了计算MFCCs，我们取音频波形的一小段时间片并计算快速傅里叶变换。
Dialogue: 0,0:09:13.58,0:09:17.82,Default,,0,0,0,,这给我们每个频率在该时间片中的功率量。
Dialogue: 0,0:09:17.80,0:09:22.01,Default,,0,0,0,,然后我们将一组滤波器应用于快速傅里叶变换频谱。
Dialogue: 0,0:09:22.07,0:09:27.45,Default,,0,0,0,,注意，这些滤波器的间隔方式代表了人类感知声音的方式。
Dialogue: 0,0:09:27.46,0:09:34.04,Default,,0,0,0,,通常，滤波器在1千赫以下是线性间隔，在1千赫以上是对数间隔。
Dialogue: 0,0:09:34.06,0:09:39.64,Default,,0,0,0,,然后我们将每个滤波器中的功率相加，得到一个代表该滤波器下能量的数字。
Dialogue: 0,0:09:39.68,0:09:44.64,Default,,0,0,0,,注意，大多数MFCC实现使用26个滤波器来处理语音。
Dialogue: 0,0:09:44.65,0:09:48.73,Default,,0,0,0,,然后，你需要对向量中的每个值取对数。
Dialogue: 0,0:09:48.79,0:09:54.90,Default,,0,0,0,,之后，我们计算26个对数滤波器能量的离散余弦变换。
Dialogue: 0,0:09:54.98,0:10:04.23,Default,,0,0,0,,DCT的工作方式类似于傅里叶变换，但它作用于实数信号，并且更好地强调低频分量。
Dialogue: 0,0:10:04.36,0:10:07.88,Default,,0,0,0,,如果你从26个滤波器能量元素开始，
Dialogue: 0,0:10:07.87,0:10:11.15,Default,,0,0,0,,你应该会得到26个倒谱系数。
Dialogue: 0,0:10:11.23,0:10:20.41,Default,,0,0,0,,较低的系数，如元素0、1和2，包含该时间片中音频频谱的一般形状信息。
Dialogue: 0,0:10:20.53,0:10:25.73,Default,,0,0,0,,随着系数的增加，你会开始得到音频频谱的更精细细节。
Dialogue: 0,0:10:25.77,0:10:32.13,Default,,0,0,0,,对于语音分析，你通常希望丢弃第零元素和第13元素之后的所有元素。
Dialogue: 0,0:10:32.18,0:10:37.93,Default,,0,0,0,,第13元素以上的通常是噪音和音频伪影，与语音相关性不大。
Dialogue: 0,0:10:38.01,0:10:43.21,Default,,0,0,0,,因此，我们计算了音频文件第一个时间片的MFCCs。
Dialogue: 0,0:10:43.48,0:10:49.69,Default,,0,0,0,,注意，我将向量翻转，使第一个元素在底部，最高的元素在顶部。
Dialogue: 0,0:10:49.71,0:10:51.18,Default,,0,0,0,,你马上就会明白为什么。
Dialogue: 0,0:10:51.20,0:10:57.42,Default,,0,0,0,,此外，经过一些测试，我发现对于我计划训练的模型，16个元素效果最好。
Dialogue: 0,0:10:57.53,0:11:00.30,Default,,0,0,0,,注意，这可能不是普遍适用的。
Dialogue: 0,0:11:00.34,0:11:06.57,Default,,0,0,0,,你可能能够使用不同的神经网络和只有12个元素来获得良好的或更好的准确性。
Dialogue: 0,0:11:06.62,0:11:10.94,Default,,0,0,0,,关键是不断尝试和实验，找到有效的东西。
Dialogue: 0,0:11:11.00,0:11:17.48,Default,,0,0,0,,然后我们在波形上滑动窗口，并从该时间片计算MFCCs。
Dialogue: 0,0:11:17.51,0:11:21.81,Default,,0,0,0,,继续进行，直到我们获得整个波形的所有MFCCs。
Dialogue: 0,0:11:21.88,0:11:26.45,Default,,0,0,0,,此时，我们有一个二维的MFCC值数组。
Dialogue: 0,0:11:26.42,0:11:29.36,Default,,0,0,0,,一种查看这个矩阵的方法是将其视为图像。
Dialogue: 0,0:11:29.41,0:11:31.85,Default,,0,0,0,,这里的x轴是时间。
Dialogue: 0,0:11:31.84,0:11:36.79,Default,,0,0,0,,每列像素对应我们取的一个时间片。
Dialogue: 0,0:11:36.93,0:11:42.33,Default,,0,0,0,,y轴是MFCCs，所以应该总共有16行。
Dialogue: 0,0:11:42.35,0:11:47.37,Default,,0,0,0,,颜色为我们提供了每个系数值的相对表示。
Dialogue: 0,0:11:47.42,0:11:54.18,Default,,0,0,0,,最底行或第零系数是深色的，因为它们与其余的相比都是大的负值。
Dialogue: 0,0:11:54.35,0:11:59.15,Default,,0,0,0,,这是一个人说“停”这个词时计算的MFCCs示例。
Dialogue: 0,0:11:59.14,0:12:05.51,Default,,0,0,0,,如果我们计算一个人说“零”的MFCCs，你会看到它生成的图像略有不同。
Dialogue: 0,0:12:05.68,0:12:11.17,Default,,0,0,0,,人类可能很难看到这些差异，但机器可以轻松做到。
Dialogue: 0,0:12:11.12,0:12:17.34,Default,,0,0,0,,所以我们实际上将使用一个神经网络来帮助区分这些不同的唤醒词。
Dialogue: 0,0:12:17.48,0:12:23.35,Default,,0,0,0,,如果你想了解更多关于MFCCs的信息，请查看Practical Cryptography网站上的这篇文章。
Dialogue: 0,0:12:23.43,0:12:29.79,Default,,0,0,0,,事实上，这篇文章的作者也是我们将使用的Python Speech Features库的作者。
Dialogue: 0,0:12:29.79,0:12:31.31,Default,,0,0,0,,让我们回到代码。
Dialogue: 0,0:12:31.29,0:12:38.65,Default,,0,0,0,,我们将编写一个快速函数，从给定路径加载波形文件并重新采样到8000个样本每秒。
Dialogue: 0,0:12:38.71,0:12:43.63,Default,,0,0,0,,我们将使用Librosa来完成，因为它可以在一行代码中加载和重新采样。
Dialogue: 0,0:12:43.66,0:12:50.58,Default,,0,0,0,,然后我们将使用Python Speech Features的MFCC函数从该波形中创建一组MFCCs。
Dialogue: 0,0:12:50.64,0:12:52.98,Default,,0,0,0,,随意调整这些参数。
Dialogue: 0,0:12:53.02,0:13:02.10,Default,,0,0,0,,我想保持产生的MFCC集合数量较少，所以我将窗口从25毫秒扩大到256毫秒。
Dialogue: 0,0:13:02.21,0:13:06.73,Default,,0,0,0,,我还将窗口之间的距离增加到50毫秒。
Dialogue: 0,0:13:06.72,0:13:12.46,Default,,0,0,0,,我们需要从中获得前16个MFCCs，并使用默认的26个滤波器。
Dialogue: 0,0:13:12.61,0:13:17.20,Default,,0,0,0,,用于FFT的样本数取决于窗口的大小，
Dialogue: 0,0:13:17.20,0:13:21.66,Default,,0,0,0,,所以我将它从默认的512增加到2048。
Dialogue: 0,0:13:21.76,0:13:26.60,Default,,0,0,0,,我发现预加重滤波器没有太大影响，所以我禁用了它。
Dialogue: 0,0:13:26.70,0:13:28.62,Default,,0,0,0,,你可能会有更好的运气。
Dialogue: 0,0:13:28.68,0:13:35.25,Default,,0,0,0,,据说你可以对最终系数添加liftering操作，以帮助它们更能抵抗噪音。
Dialogue: 0,0:13:35.26,0:13:41.08,Default,,0,0,0,,这是我在这个基本原型中不需要的另一操作，所以我禁用了liftering。
Dialogue: 0,0:13:41.27,0:13:45.41,Default,,0,0,0,,由于MFCCs的零宽元素通常被丢弃，
Dialogue: 0,0:13:45.40,0:13:50.84,Default,,0,0,0,,该函数可以选择用一个表示该帧总能量的值来替换它们。
Dialogue: 0,0:13:50.82,0:13:55.90,Default,,0,0,0,,在这个原型中我发现这并不必要，所以我关闭了这个功能。
Dialogue: 0,0:13:56.00,0:14:06.48,Default,,0,0,0,,最后，应用像Hamming或Hanning窗口这样的窗口函数可以帮助防止快速傅里叶变换操作在高频率中产生不必要的伪影。
Dialogue: 0,0:14:06.52,0:14:08.50,Default,,0,0,0,,让我们在一些文件上测试一下。
Dialogue: 0,0:14:08.48,0:14:14.55,Default,,0,0,0,,我将从训练集中的前500个样本中获取并显示其MFCC矩阵的形状。
Dialogue: 0,0:14:14.70,0:14:19.35,Default,,0,0,0,,每个音频文件应该产生16组16个系数。
Dialogue: 0,0:14:19.43,0:14:25.67,Default,,0,0,0,,如你所见，一些音频文件似乎已损坏或不足一秒长。
Dialogue: 0,0:14:25.69,0:14:28.36,Default,,0,0,0,,如果我们将这些数据计算并除以500，
Dialogue: 0,0:14:28.31,0:14:33.14,Default,,0,0,0,,我们可以得出结论，约10%的音频样本有这个问题。
Dialogue: 0,0:14:33.30,0:14:35.08,Default,,0,0,0,,让我们测试一些。
Dialogue: 0,0:14:35.18,0:14:43.11,Default,,0,0,0,,这是一个快速脚本，使用play sound库播放音频样本，显示MFCCs和生成的图像，
Dialogue: 0,0:14:43.08,0:14:45.98,Default,,0,0,0,,并告诉我们它应该是什么词。
Dialogue: 0,0:14:46.10,0:14:47.82,Default,,0,0,0,,在测试了许多这些样本后，
Dialogue: 0,0:14:47.80,0:14:51.69,Default,,0,0,0,,我发现许多样本被切断或完全无法听见。
Dialogue: 0,0:14:52.67,0:14:56.81,Default,,0,0,0,,在有问题的数据集中处理不良数据有多种方法。
Dialogue: 0,0:14:56.91,0:14:59.11,Default,,0,0,0,,如果样本不够长，
Dialogue: 0,0:14:59.10,0:15:02.74,Default,,0,0,0,,你可以追加看起来像样本内数据的值，
Dialogue: 0,0:15:02.71,0:15:05.79,Default,,0,0,0,,比如接近静音或白噪声的东西。
Dialogue: 0,0:15:05.85,0:15:10.41,Default,,0,0,0,,你也可以完全删除样本，这是最简单的做法。
Dialogue: 0,0:15:10.41,0:15:14.57,Default,,0,0,0,,由于这个数据集中只有约10%的样本有问题，
Dialogue: 0,0:15:14.56,0:15:19.84,Default,,0,0,0,,所以我将删除任何未生成准确16组系数的样本。
Dialogue: 0,0:15:19.88,0:15:23.36,Default,,0,0,0,,所以我们编写了一个函数来做这件事。
Dialogue: 0,0:15:23.35,0:15:25.93,Default,,0,0,0,,它确保文件以.WAVE结尾，
Dialogue: 0,0:15:25.94,0:15:32.78,Default,,0,0,0,,计算MFCCs并在样本不足长时从Y向量中删除样本及其对应的标签。
Dialogue: 0,0:15:32.93,0:15:37.99,Default,,0,0,0,,然后我们在每个训练集、验证集和测试集上运行该函数。
Dialogue: 0,0:15:38.00,0:15:41.88,Default,,0,0,0,,这可能需要几分钟，所以去做个三明治吧。
Dialogue: 0,0:15:41.90,0:15:47.95,Default,,0,0,0,,完成后，你会看到它从集合中删除了约10%的样本，我对此没有问题。
Dialogue: 0,0:15:48.10,0:15:54.82,Default,,0,0,0,,最后，我们使用NumPy的saveZ函数将这些巨大的数组存储到一个NPZ文件中。
Dialogue: 0,0:15:54.80,0:16:02.78,Default,,0,0,0,,这将允许我们在未来步骤中加载保存的特征和相应的标签，当我们准备进行实际机器学习时。
Dialogue: 0,0:16:02.81,0:16:07.92,Default,,0,0,0,,要加载特征，只需调用numpy.load并给出文件位置。
Dialogue: 0,0:16:07.95,0:16:12.63,Default,,0,0,0,,然后我们可以列出可用的数组并查看每个数组中的样本数量。
Dialogue: 0,0:16:12.63,0:16:18.19,Default,,0,0,0,,如果我们打印Y验证集，你会看到我们在该集中存储的所有标签。
Dialogue: 0,0:16:18.17,0:16:25.25,Default,,0,0,0,,下次我们将使用这些存储的特征来训练一个神经网络，将我们的唤醒词与所有其他词区分开来。
Dialogue: 0,0:16:25.25,0:16:30.22,Default,,0,0,0,,记住，这不是一个非常健壮的模型，这只是一个开始。
Dialogue: 0,0:16:30.25,0:16:34.04,Default,,0,0,0,,如果你想继续观看这些视频，请订阅，祝你黑客快乐。
