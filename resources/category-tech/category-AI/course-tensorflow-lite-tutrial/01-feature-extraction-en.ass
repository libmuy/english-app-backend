[Script Info]
; Script generated by Aegisub 9706-cibuilds-20caaabc0
; http://www.aegisub.org/
Title: Default Aegisub file
ScriptType: v4.00+
WrapStyle: 0
ScaledBorderAndShadow: yes
YCbCr Matrix: None

[Aegisub Project Garbage]
Audio File: part1.wav
Video Zoom Percent: 1.000000
Active Line: 208

[V4+ Styles]
Format: Name, Fontname, Fontsize, PrimaryColour, SecondaryColour, OutlineColour, BackColour, Bold, Italic, Underline, StrikeOut, ScaleX, ScaleY, Spacing, Angle, BorderStyle, Outline, Shadow, Alignment, MarginL, MarginR, MarginV, Encoding
Style: Default,Arial,48,&H00FFFFFF,&H000000FF,&H00000000,&H00000000,0,0,0,0,100,100,0,0,1,2,2,2,10,10,10,1

[Events]
Format: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text
Dialogue: 0,0:00:00.00,0:00:03.82,Default,,0,0,0,,Over the last month, I've been playing around with TensorFlow and Keras, 
Dialogue: 0,0:00:03.82,0:00:14.14,Default,,0,0,0,,and I've noticed that a lot of the examples are these pre-canned demos that are great for showing you various features  but don't really show you how to use either of the frameworks.
Dialogue: 0,0:00:14.15,0:00:18.81,Default,,0,0,0,,I'm hoping to change that without diving too deep into the guts of machine learning.
Dialogue: 0,0:00:18.81,0:00:23.99,Default,,0,0,0,,Over the next few episodes, I'm going to show you how to extract features from voice commands,
Dialogue: 0,0:00:23.99,0:00:31.31,Default,,0,0,0,,use them to train a deep neural network, and then load that neural network onto a Raspberry Pi using TensorFlow Lite.
Dialogue: 0,0:00:31.33,0:00:39.24,Default,,0,0,0,,The idea is that we'll have a Pi constantly listening for audio, and when it hears our wake word, stop, it will toggle a pin.
Dialogue: 0,0:00:39.29,0:00:42.52,Default,,0,0,0,,Please note that I'm not in any way a machine learning expert.
Dialogue: 0,0:00:42.52,0:00:46.69,Default,,0,0,0,,I'm still learning this as I go, but I wanted to share what I've found so far.
Dialogue: 0,0:00:46.71,0:00:50.49,Default,,0,0,0,,If you know of any better methods or techniques, please comment on this video.
Dialogue: 0,0:00:50.49,0:00:53.48,Default,,0,0,0,,I'd love to know if you think there's a better way to do things.
Dialogue: 0,0:00:53.58,0:00:57.45,Default,,0,0,0,,Most machine learning problems can be broken down into four steps.
Dialogue: 0,0:00:57.46,0:01:01.02,Default,,0,0,0,,The first is to collect a bunch of data that you think you'll need.
Dialogue: 0,0:01:01.04,0:01:08.08,Default,,0,0,0,,For the wake word, or hot word detection, we'll need a bunch of people saying many different words, including our target word.
Dialogue: 0,0:01:08.10,0:01:11.41,Default,,0,0,0,,The good news is that Google has already done this for us.
Dialogue: 0,0:01:11.42,0:01:14.99,Default,,0,0,0,,We can use their speech commands data set to make life easier.
Dialogue: 0,0:01:14.99,0:01:21.40,Default,,0,0,0,,Next, we need to figure out which features we can use in this data to discern one command from another.
Dialogue: 0,0:01:21.43,0:01:23.79,Default,,0,0,0,,After that, we'll build and train a model.
Dialogue: 0,0:01:23.80,0:01:33.00,Default,,0,0,0,,Finally, we deploy that model to the Raspberry Pi, where we'll use TensorFlow Lite to help us determine if the target word was spoken in real time.
Dialogue: 0,0:01:33.09,0:01:37.43,Default,,0,0,0,,For this episode, I'd like to focus just on feature extraction.
Dialogue: 0,0:01:37.44,0:01:40.65,Default,,0,0,0,,Note that you can't really use TensorFlow Lite to train a model.
Dialogue: 0,0:01:40.65,0:01:46.10,Default,,0,0,0,,Its main purpose is to make predictions and inferences once you do have a fully trained model.
Dialogue: 0,0:01:46.12,0:01:52.30,Default,,0,0,0,,So for these first two episodes, we'll be working with Keras and TensorFlow in our desktop or laptop.
Dialogue: 0,0:01:52.31,0:01:55.98,Default,,0,0,0,,In fact, we won't even need TensorFlow until the next episode, 
Dialogue: 0,0:01:55.98,0:02:00.98,Default,,0,0,0,,and that's because we're going to be focused on just extracting features from audio data.
Dialogue: 0,0:02:00.99,0:02:07.99,Default,,0,0,0,,If you have not done so already, I recommend taking a look at my video for installing TensorFlow, Keras, and Jupyter Notebook.
Dialogue: 0,0:02:08.00,0:02:10.18,Default,,0,0,0,,We'll be needing them throughout this process.
Dialogue: 0,0:02:10.18,0:02:17.83,Default,,0,0,0,,If you have a Google account, you can also head to colab.research.google.com to run Jupyter Notebook in the cloud.
Dialogue: 0,0:02:17.84,0:02:20.50,Default,,0,0,0,,There is no setup involved and it's free.
Dialogue: 0,0:02:20.51,0:02:27.96,Default,,0,0,0,,Note that all files are stored on your Google Drive, so it might take some extra work to move very large datasets in and out of your workspace.
Dialogue: 0,0:02:27.97,0:02:33.85,Default,,0,0,0,,Also note that loading files into Colab requires different commands than doing it locally on your own machine.
Dialogue: 0,0:02:33.95,0:02:36.75,Default,,0,0,0,,To start, we need to download the dataset.
Dialogue: 0,0:02:36.71,0:02:39.14,Default,,0,0,0,,Search for the Google Speech Commands dataset.
Dialogue: 0,0:02:39.30,0:02:43.62,Default,,0,0,0,,I've had the best luck finding it on the TensorFlow GitHub repo docs page.
Dialogue: 0,0:02:43.60,0:02:48.82,Default,,0,0,0,,I'll grab version 0.02, which seems to have worked well for this application.
Dialogue: 0,0:02:48.85,0:02:55.08,Default,,0,0,0,,Note that it's a pretty big download, and if you want, you can contribute your own voice to the dataset while you wait.
Dialogue: 0,0:02:55.11,0:02:58.87,Default,,0,0,0,,When it's done downloading, unzip the files to somewhere on your computer.
Dialogue: 0,0:02:58.89,0:03:02.83,Default,,0,0,0,,I'll keep mine in a datasets folder near my other Python projects.
Dialogue: 0,0:03:02.98,0:03:05.50,Default,,0,0,0,,Feel free to browse through the files in the dataset.
Dialogue: 0,0:03:05.51,0:03:07.67,Default,,0,0,0,,We'll pick stop as our wake word.
Dialogue: 0,0:03:07.70,0:03:13.06,Default,,0,0,0,,If we play them, it's just a collection of recordings from various people saying, stop.
Dialogue: 0,0:03:13.22,0:03:17.38,Default,,0,0,0,,You'll want to copy down the absolute path to your Speech Commands dataset.
Dialogue: 0,0:03:17.43,0:03:18.75,Default,,0,0,0,,We'll need it in a minute.
Dialogue: 0,0:03:18.80,0:03:23.70,Default,,0,0,0,,If you want to create your own wake word, you'll first need to make a new folder in this dataset.
Dialogue: 0,0:03:23.78,0:03:27.31,Default,,0,0,0,,Then record yourself or other people saying that wake word.
Dialogue: 0,0:03:27.34,0:03:32.28,Default,,0,0,0,,You'll want the audio clips to be exactly one second for them to work in the rest of this project.
Dialogue: 0,0:03:32.39,0:03:35.65,Default,,0,0,0,,The bigger variety of audio you can get, the better.
Dialogue: 0,0:03:35.62,0:03:39.51,Default,,0,0,0,,Ideally, you'll want at least a few hundred different recordings.
Dialogue: 0,0:03:39.51,0:03:44.27,Default,,0,0,0,,There's a good bit of code involved in these steps, so I'm going to briefly go over each chunk of code.
Dialogue: 0,0:03:44.26,0:03:48.65,Default,,0,0,0,,If you'd like to take your time to go over the code, you can find it in a link in the description.
Dialogue: 0,0:03:48.68,0:03:53.46,Default,,0,0,0,,Start a new Jupyter Notebook session using TensorFlow or TensorFlow GPU.
Dialogue: 0,0:03:53.45,0:03:56.81,Default,,0,0,0,,In a new notebook, let's import the necessary packages.
Dialogue: 0,0:03:56.79,0:04:02.30,Default,,0,0,0,,We'll need OS for dealing with files, Librosa for loading and resampling WAV files, 
Dialogue: 0,0:04:02.31,0:04:08.40,Default,,0,0,0,,Random for shuffling data, NumPy for dealing with matrices, Matplotlib for graphing things,
Dialogue: 0,0:04:08.40,0:04:12.20,Default,,0,0,0,,and Python Speech Features for extracting audio features.
Dialogue: 0,0:04:12.20,0:04:14.75,Default,,0,0,0,,Note that if you do not have one of these packages, 
Dialogue: 0,0:04:14.73,0:04:20.81,Default,,0,0,0,,you can install it from within Jupyter Notebook by typing exclamation point pip install and the name of the package.
Dialogue: 0,0:04:20.84,0:04:24.87,Default,,0,0,0,,From there, we'll use join to construct the path to our data set.
Dialogue: 0,0:04:24.88,0:04:28.80,Default,,0,0,0,,Let's print them out to make sure we can see all of the speech directories.
Dialogue: 0,0:04:28.93,0:04:33.37,Default,,0,0,0,,We can pick just a few words to train with, but I'm going to train with all of the words.
Dialogue: 0,0:04:33.38,0:04:36.82,Default,,0,0,0,,We'll tell it to recognize only one of the words in a later step.
Dialogue: 0,0:04:36.91,0:04:40.83,Default,,0,0,0,,The background noise seems to mess things up for me, so I'm going to leave it off.
Dialogue: 0,0:04:40.85,0:04:48.82,Default,,0,0,0,,In theory, you could mix these noises in with the other speech samples to train a model that could automatically filter out various background sounds.
Dialogue: 0,0:04:48.83,0:04:52.27,Default,,0,0,0,,Let's print out the number of files in each target directory.
Dialogue: 0,0:04:52.31,0:04:56.07,Default,,0,0,0,,Each word has a few thousand samples, as you can see.
Dialogue: 0,0:04:56.07,0:04:58.91,Default,,0,0,0,,Let's set a few parameters for the rest of this script.
Dialogue: 0,0:04:58.88,0:05:04.66,Default,,0,0,0,,We'll want to create features for all of the target words, even if we just pick out one target word later. 
Dialogue: 0,0:05:04.65,0:05:07.84,Default,,0,0,0,,At the end of the script, we'll have a collection of features,
Dialogue: 0,0:05:07.81,0:05:13.50,Default,,0,0,0,,essentially matrices that resemble images, that will store into an NPZ file.
Dialogue: 0,0:05:13.65,0:05:17.29,Default,,0,0,0,,In practice, you'd want to use all the data you have available.
Dialogue: 0,0:05:17.29,0:05:22.89,Default,,0,0,0,,However, it can take hours to extract features and train using 100,000 samples.
Dialogue: 0,0:05:22.92,0:05:26.94,Default,,0,0,0,,So I find it much easier to work with a random subset of data.
Dialogue: 0,0:05:26.92,0:05:28.31,Default,,0,0,0,,I'll use 10%.
Dialogue: 0,0:05:28.38,0:05:33.67,Default,,0,0,0,,This should only be used for initial prototypes to make sure that things are working.
Dialogue: 0,0:05:33.76,0:05:38.52,Default,,0,0,0,,You'll want to go back to the full 100% of data when you train your final model.
Dialogue: 0,0:05:38.59,0:05:44.23,Default,,0,0,0,,We also want to set aside 10% of our data for cross-validation and 10% for testing.
Dialogue: 0,0:05:44.23,0:05:45.95,Default,,0,0,0,,I'll talk about that in a minute.
Dialogue: 0,0:05:45.95,0:05:50.23,Default,,0,0,0,,While the WAV files are recorded with a 16 kHz sampling rate,
Dialogue: 0,0:05:50.27,0:05:57.05,Default,,0,0,0,,we'll be able to get our final model to run faster if we can use a lower sampling rate, like 8 kHz.
Dialogue: 0,0:05:57.08,0:06:04.51,Default,,0,0,0,,We'll set the number of Mell frequency sepstral coefficients to 16 and the length of these MFCCs to 16.
Dialogue: 0,0:06:04.50,0:06:11.37,Default,,0,0,0,,I'll also talk about why MFCCs make for good features in a minute, but let's continue with getting raw data from our files.
Dialogue: 0,0:06:11.38,0:06:15.64,Default,,0,0,0,,Next, we're going to create a list of all the file names with their full path.
Dialogue: 0,0:06:15.74,0:06:19.85,Default,,0,0,0,,This will allow us to load each one and extract the features automatically.
Dialogue: 0,0:06:19.83,0:06:23.06,Default,,0,0,0,,In addition, we want to create a Y array.
Dialogue: 0,0:06:23.05,0:06:26.78,Default,,0,0,0,,This array holds the ground truth or actual values.
Dialogue: 0,0:06:26.88,0:06:31.44,Default,,0,0,0,,Since this is a supervised learning exercise where we classify signals,
Dialogue: 0,0:06:31.47,0:06:34.68,Default,,0,0,0,,we'll need the labels for the signals during the training step.
Dialogue: 0,0:06:34.80,0:06:38.63,Default,,0,0,0,,We can arbitrarily assign values, but they should be consistent.
Dialogue: 0,0:06:38.67,0:06:41.72,Default,,0,0,0,,We'll assign numbers to the words in alphabetical order.
Dialogue: 0,0:06:41.81,0:06:46.50,Default,,0,0,0,,Backward is 0, bed is 1, bird is 2, and so on.
Dialogue: 0,0:06:46.56,0:06:47.96,Default,,0,0,0,,Let's print out Y.
Dialogue: 0,0:06:47.96,0:06:50.19,Default,,0,0,0,,We see that it's a collection of arrays,
Dialogue: 0,0:06:50.16,0:06:54.54,Default,,0,0,0,,and each array is simply the number we assigned to the target word.
Dialogue: 0,0:06:54.61,0:06:59.59,Default,,0,0,0,,So there are 1664 zeros in this first array,
Dialogue: 0,0:06:59.58,0:07:05.82,Default,,0,0,0,,which correspond to the 1664 samples of people saying the word backward.
Dialogue: 0,0:07:05.89,0:07:09.83,Default,,0,0,0,,Similarly, there are 2014 ones in the next array,
Dialogue: 0,0:07:09.80,0:07:12.54,Default,,0,0,0,,which correspond to the samples of bed.
Dialogue: 0,0:07:12.68,0:07:18.15,Default,,0,0,0,,We then flatten these arrays so they're just one long list rather than a collection of arrays.
Dialogue: 0,0:07:18.16,0:07:21.61,Default,,0,0,0,,The next part is easiest to describe with a diagram.
Dialogue: 0,0:07:21.73,0:07:27.66,Default,,0,0,0,,We'll use the Python zip command to link each file name with its associated Y value.
Dialogue: 0,0:07:27.75,0:07:31.55,Default,,0,0,0,,There are a lot more files than this, but go with me here.
Dialogue: 0,0:07:31.59,0:07:33.83,Default,,0,0,0,,We then randomly shuffle the file names,
Dialogue: 0,0:07:33.83,0:07:37.99,Default,,0,0,0,,and notice that the Y values stayed linked to the individual names.
Dialogue: 0,0:07:38.06,0:07:44.06,Default,,0,0,0,,We can then unzip the two lists to separate file names and Y, but their ordering will remain.
Dialogue: 0,0:07:44.12,0:07:50.15,Default,,0,0,0,,Finally, we will take the first 10% of the data and set it aside to be our cross-validation set.
Dialogue: 0,0:07:50.20,0:07:54.92,Default,,0,0,0,,This will be useful in testing the model to see how well it performs during training.
Dialogue: 0,0:07:54.91,0:07:58.35,Default,,0,0,0,,Then we set aside another 10% as test data.
Dialogue: 0,0:07:58.35,0:08:03.13,Default,,0,0,0,,We should only test with this data when we're done tweaking the model and hyperparameters.
Dialogue: 0,0:08:03.20,0:08:06.96,Default,,0,0,0,,We leave the rest of the data alone to be used as training data.
Dialogue: 0,0:08:07.12,0:08:12.68,Default,,0,0,0,,Note that cross-validation and test sets can be anywhere from 10% to 20% of the data each.
Dialogue: 0,0:08:12.84,0:08:17.35,Default,,0,0,0,,Feel free to pick something in that range depending on how much total data you have available.
Dialogue: 0,0:08:17.34,0:08:21.42,Default,,0,0,0,,Back to our code, we can see how those steps were accomplished in these cells.
Dialogue: 0,0:08:21.39,0:08:25.86,Default,,0,0,0,,We zip the file names and Y array together, shuffle them and unzip them.
Dialogue: 0,0:08:25.85,0:08:29.20,Default,,0,0,0,,Because I don't want to take a lot of time training a prototype model,
Dialogue: 0,0:08:29.19,0:08:32.71,Default,,0,0,0,,I'm going to only work with 10% of the total data for now.
Dialogue: 0,0:08:32.70,0:08:39.06,Default,,0,0,0,,It's really important that you come back to this and use all the data when you're ready to train your model for testing and deployment.
Dialogue: 0,0:08:39.14,0:08:45.68,Default,,0,0,0,,Here, we break apart the file name list and ground truth list into separate validation, test and training sets.
Dialogue: 0,0:08:45.84,0:08:48.96,Default,,0,0,0,,We're now ready to extract features from these WAV files.
Dialogue: 0,0:08:49.00,0:08:54.07,Default,,0,0,0,,When it comes to feature extraction, it's a good idea to do some research on what other people have done.
Dialogue: 0,0:08:54.12,0:08:58.38,Default,,0,0,0,,This is where I found out about MFCCs and from what I read,
Dialogue: 0,0:08:58.39,0:09:02.49,Default,,0,0,0,,transforming audio into the Mell frequency sepstral coefficients
Dialogue: 0,0:09:02.47,0:09:06.30,Default,,0,0,0,,seems to be very popular in machine learning for speech recognition.
Dialogue: 0,0:09:06.28,0:09:13.58,Default,,0,0,0,,To calculate the MFCCs, we take a small time slice of our audio waveform and compute the fast Fourier transform.
Dialogue: 0,0:09:13.58,0:09:17.82,Default,,0,0,0,,This gives us the amount of power at each frequency of that time slice.
Dialogue: 0,0:09:17.80,0:09:22.01,Default,,0,0,0,,Then we apply a set of filters to that fast Fourier transform spectrum.
Dialogue: 0,0:09:22.07,0:09:27.45,Default,,0,0,0,,Note that these filters are spaced in such a way to represent how humans perceive sound.
Dialogue: 0,0:09:27.46,0:09:34.04,Default,,0,0,0,,Generally, the filters are linearly spaced below 1 kilohertz and logarithmically spaced above 1 kilohertz.
Dialogue: 0,0:09:34.06,0:09:39.64,Default,,0,0,0,,We then sum up the power found in each filter to get a number representing the energy under that filter.
Dialogue: 0,0:09:39.68,0:09:44.64,Default,,0,0,0,,Note that most implementations of MFCC use 26 filters for voice.
Dialogue: 0,0:09:44.65,0:09:48.73,Default,,0,0,0,,From there, you'll want to compute the log of each value in the vector.
Dialogue: 0,0:09:48.79,0:09:54.90,Default,,0,0,0,,After that, we compute the discrete cosine transform of the 26 log filter bank energies.
Dialogue: 0,0:09:54.98,0:10:04.23,Default,,0,0,0,,The DCT works much like the Fourier transform but operates on real valued signals and does a better job of emphasizing the low frequency components.
Dialogue: 0,0:10:04.36,0:10:07.88,Default,,0,0,0,,If you start with 26 elements in the filter bank energies,
Dialogue: 0,0:10:07.87,0:10:11.15,Default,,0,0,0,,you should end up with 26 sepstral coefficients.
Dialogue: 0,0:10:11.23,0:10:20.41,Default,,0,0,0,,The lower coefficients, like elements 0, 1, and 2, contain information about the general shape of the audio spectrum in that time slice.
Dialogue: 0,0:10:20.53,0:10:25.73,Default,,0,0,0,,As you go up in the coefficients, you start to get into the finer details of the audio spectrum.
Dialogue: 0,0:10:25.77,0:10:32.13,Default,,0,0,0,,For speech analysis, you normally want to throw away the zeroth element and anything after element 13.
Dialogue: 0,0:10:32.18,0:10:37.93,Default,,0,0,0,,Above the 13th element is usually noise and audio artifacts that don't correlate to speech much.
Dialogue: 0,0:10:38.01,0:10:43.21,Default,,0,0,0,,So we've computed the MFCCs for the first time slice of our audio file.
Dialogue: 0,0:10:43.48,0:10:49.69,Default,,0,0,0,,Notice that I've flipped the vector around so that the first element is on the bottom and the highest element is on the top.
Dialogue: 0,0:10:49.71,0:10:51.18,Default,,0,0,0,,You'll see why in a second.
Dialogue: 0,0:10:51.20,0:10:57.42,Default,,0,0,0,,Also, after some testing, I found that for the model I plan to train, having 16 elements seems to work the best.
Dialogue: 0,0:10:57.53,0:11:00.30,Default,,0,0,0,,Note that this might not be universally true.
Dialogue: 0,0:11:00.34,0:11:06.57,Default,,0,0,0,,You might be able to get good or better accuracy with a different neural network and only 12 elements.
Dialogue: 0,0:11:06.62,0:11:10.94,Default,,0,0,0,,The point is to keep trying and experimenting with things to find something that works.
Dialogue: 0,0:11:11.00,0:11:17.48,Default,,0,0,0,,We then slide our window over a bit on the waveform and compute the MFCCs from that time slice.
Dialogue: 0,0:11:17.51,0:11:21.81,Default,,0,0,0,,Keep going until we've obtained all the MFCCs for the whole waveform.
Dialogue: 0,0:11:21.88,0:11:26.45,Default,,0,0,0,,At this point, we have a two-dimensional array of MFCC values.
Dialogue: 0,0:11:26.42,0:11:29.36,Default,,0,0,0,,One way to view this matrix is as an image.
Dialogue: 0,0:11:29.41,0:11:31.85,Default,,0,0,0,,The x-axis here is the time.
Dialogue: 0,0:11:31.84,0:11:36.79,Default,,0,0,0,,Each column of pixels corresponds to one of the time slices we took.
Dialogue: 0,0:11:36.93,0:11:42.33,Default,,0,0,0,,The y-axis is the MFCCs, so there should be 16 total rows.
Dialogue: 0,0:11:42.35,0:11:47.37,Default,,0,0,0,,The colors give us a relative representation of the value of each coefficient.
Dialogue: 0,0:11:47.42,0:11:54.18,Default,,0,0,0,,The bottom row, or zeroth coefficient, is dark because they're all large negative values compared to the rest.
Dialogue: 0,0:11:54.35,0:11:59.15,Default,,0,0,0,,This is an example of MFCCs computed from someone saying the word stop.
Dialogue: 0,0:11:59.14,0:12:05.51,Default,,0,0,0,,If we compute the MFCCs for someone saying zero, you can see how it produces a slightly different image.
Dialogue: 0,0:12:05.68,0:12:11.17,Default,,0,0,0,,It might be tough for a human to see these differences, but a machine can do that quite easily.
Dialogue: 0,0:12:11.12,0:12:17.34,Default,,0,0,0,,So we will actually use a neural network that classifies images to help discern these different wake words.
Dialogue: 0,0:12:17.48,0:12:23.35,Default,,0,0,0,,If you'd like to learn more about MFCCs, check out this post on the Practical Cryptography site.
Dialogue: 0,0:12:23.43,0:12:29.79,Default,,0,0,0,,In fact, this post was written by the same author of the Python Speech Features Library that we'll be using.
Dialogue: 0,0:12:29.79,0:12:31.31,Default,,0,0,0,,Let's go back to our code.
Dialogue: 0,0:12:31.29,0:12:38.65,Default,,0,0,0,,We'll make a quick function that loads the wave file from a given path and resamples it to 8,000 samples per second.
Dialogue: 0,0:12:38.71,0:12:43.63,Default,,0,0,0,,We'll use Librosa to do that, since it can load and resample in one line of code.
Dialogue: 0,0:12:43.66,0:12:50.58,Default,,0,0,0,,Then we'll use the Python Speech Features MFCC function to create a set of MFCCs from that waveform.
Dialogue: 0,0:12:50.64,0:12:52.98,Default,,0,0,0,,Feel free to play around with these parameters.
Dialogue: 0,0:12:53.02,0:13:02.10,Default,,0,0,0,,I wanted to keep the number of MFCC sets produced low, so I widened the window from 25 milliseconds to 256 milliseconds.
Dialogue: 0,0:13:02.21,0:13:06.73,Default,,0,0,0,,I also increased the distance between the windows to 50 milliseconds.
Dialogue: 0,0:13:06.72,0:13:12.46,Default,,0,0,0,,We want the first 16 MFCCs from this, and we'll use the default 26 filters.
Dialogue: 0,0:13:12.61,0:13:17.20,Default,,0,0,0,,The number of samples to use for the FFT is dependent on the size of the window,
Dialogue: 0,0:13:17.20,0:13:21.66,Default,,0,0,0,,so I had to increase that to 2,048 from the default 512.
Dialogue: 0,0:13:21.76,0:13:26.60,Default,,0,0,0,,I found that the pre-emphasis filter didn't make too much of a difference, so I disabled it.
Dialogue: 0,0:13:26.70,0:13:28.62,Default,,0,0,0,,You might have better luck with it on.
Dialogue: 0,0:13:28.68,0:13:35.25,Default,,0,0,0,,You can supposedly add a liftering operation on the final coefficients to help make them more robust against noise.
Dialogue: 0,0:13:35.26,0:13:41.08,Default,,0,0,0,,That's one more operation that I didn't need in this basic prototype, so I disabled liftering.
Dialogue: 0,0:13:41.27,0:13:45.41,Default,,0,0,0,,Since the zero-width elements of the MFCCs are often thrown out,
Dialogue: 0,0:13:45.40,0:13:50.84,Default,,0,0,0,,this function can offer to replace them with something that gives total energy in that frame.
Dialogue: 0,0:13:50.82,0:13:55.90,Default,,0,0,0,,Once again, I didn't find that necessary with this prototype, so I turned that feature off.
Dialogue: 0,0:13:56.00,0:14:06.48,Default,,0,0,0,,Finally, applying a window function like a Hamming or Hanning window can help prevent the Fast Fourier Transform operation produce unwanted artifacts in the higher frequencies.
Dialogue: 0,0:14:06.52,0:14:08.50,Default,,0,0,0,,Let's test this on some files.
Dialogue: 0,0:14:08.48,0:14:14.55,Default,,0,0,0,,I'll take the first 500 samples from the training set and display the shape of their MFCC matrices.
Dialogue: 0,0:14:14.70,0:14:19.35,Default,,0,0,0,,Each audio file should produce 16 sets of 16 coefficients.
Dialogue: 0,0:14:19.43,0:14:25.67,Default,,0,0,0,,As you can see, a few of the audio files seem to have been corrupted or not fully one second long.
Dialogue: 0,0:14:25.69,0:14:28.36,Default,,0,0,0,,If we count these up and divide by 500,
Dialogue: 0,0:14:28.31,0:14:33.14,Default,,0,0,0,,we can conclude that about 10% of all the audio samples have this problem.
Dialogue: 0,0:14:33.30,0:14:35.08,Default,,0,0,0,,Let's test a few of these.
Dialogue: 0,0:14:35.18,0:14:43.11,Default,,0,0,0,,Here's a quick script that uses the play sound library to play the audio sample, display the MFCCs and resulting image,
Dialogue: 0,0:14:43.08,0:14:45.98,Default,,0,0,0,,as well as give us what word it's supposed to be.
Dialogue: 0,0:14:46.10,0:14:47.82,Default,,0,0,0,,After testing a number of these,
Dialogue: 0,0:14:47.80,0:14:51.69,Default,,0,0,0,,I found that many of them were cut off or completely inaudible.
Dialogue: 0,0:14:52.67,0:14:56.81,Default,,0,0,0,,There are a number of ways to deal with bad data in a set with machine learning.
Dialogue: 0,0:14:56.91,0:14:59.11,Default,,0,0,0,,If the sample isn't quite long enough,
Dialogue: 0,0:14:59.10,0:15:02.74,Default,,0,0,0,,you can append values that look like data found within the sample,
Dialogue: 0,0:15:02.71,0:15:05.79,Default,,0,0,0,,something that approximates silence or white noise.
Dialogue: 0,0:15:05.85,0:15:10.41,Default,,0,0,0,,You can also just drop the sample completely, which is the easiest thing to do.
Dialogue: 0,0:15:10.41,0:15:14.57,Default,,0,0,0,,Since only about 10% of the samples are problematic for this data set,
Dialogue: 0,0:15:14.56,0:15:19.84,Default,,0,0,0,,I'm just going to drop any of them that don't produce exactly 16 sets of coefficients.
Dialogue: 0,0:15:19.88,0:15:23.36,Default,,0,0,0,,So we write another function that does exactly that.
Dialogue: 0,0:15:23.35,0:15:25.93,Default,,0,0,0,,It makes sure the file ends with .WAVE,
Dialogue: 0,0:15:25.94,0:15:32.78,Default,,0,0,0,,calculates the MFCCs and drops the sample and corresponding label from the Y vectors if it's not long enough.
Dialogue: 0,0:15:32.93,0:15:37.99,Default,,0,0,0,,We then run that function on each of our training, validation and test sets.
Dialogue: 0,0:15:38.00,0:15:41.88,Default,,0,0,0,,This might take a few minutes, so go make a sandwich or something.
Dialogue: 0,0:15:41.90,0:15:47.95,Default,,0,0,0,,When it's done, you can see that it removed about 10% of samples from the set, which I'm okay with.
Dialogue: 0,0:15:48.10,0:15:54.82,Default,,0,0,0,,Finally, we use the NumPy saveZ function to store these massive arrays into an NPZ file.
Dialogue: 0,0:15:54.80,0:16:02.78,Default,,0,0,0,,This will allow us to load our saved features and corresponding labels in a future step when we're ready to do the actual machine learning.
Dialogue: 0,0:16:02.81,0:16:07.92,Default,,0,0,0,,To load the features, we just call numpy.load and give it the location of the file.
Dialogue: 0,0:16:07.95,0:16:12.63,Default,,0,0,0,,We can then list out the available arrays and see the number of samples in each one.
Dialogue: 0,0:16:12.63,0:16:18.19,Default,,0,0,0,,If we print the Y validation set, you can see all of the labels that we have stored in that set.
Dialogue: 0,0:16:18.17,0:16:25.25,Default,,0,0,0,,Next time, we'll use these stored features to train a neural network that will classify our wake word from all the other words.
Dialogue: 0,0:16:25.25,0:16:30.22,Default,,0,0,0,,Remember that this isn't a very robust model, it's just something to get us started.
Dialogue: 0,0:16:30.25,0:16:34.04,Default,,0,0,0,,Please subscribe if you'd like to keep up with these videos and happy hacking.
